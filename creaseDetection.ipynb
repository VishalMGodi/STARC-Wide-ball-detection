{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shows all edges on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "something = 3\n",
    "\n",
    "# Load the video file\n",
    "# cap = cv2.VideoCapture('./Dataset/Test/Subject1.MOV')  # Replace 'cricket_pitch_video.mp4' with your video file path\n",
    "cap = cv2.VideoCapture('./Dataset/Alldone/18_Kan_C.mp4')  # Replace 'cricket_pitch_video.mp4' with your video file path\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # If the video reaches the end, reset it to the beginning\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        continue\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # cv2.imshow('Gray Scale',gray)\n",
    "\n",
    "    # # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (something, something), 0)\n",
    "    # cv2.imshow('blurred',blurred)\n",
    "\n",
    "    # # Perform Canny edge detection\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "    cv2.imshow('blurred',edges)\n",
    "\n",
    "    # # Perform line detection using HoughLinesP\n",
    "    # lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=100, maxLineGap=10)\n",
    "\n",
    "    # # Draw the detected lines on the frame\n",
    "    # if lines is not None:\n",
    "    #     for line in lines:\n",
    "    #         x1, y1, x2, y2 = line[0]\n",
    "    #         cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "    # # Display the frame with the detected crease lines\n",
    "    # cv2.imshow('Crease Lines Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detects white and batsman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "# import cvzone\n",
    "# from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "\n",
    "# Open the video file\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "cap = cv2.VideoCapture('./Dataset/Alldone/18_Kan_C.mp4')  # Replace 'video.mp4' with the path to your video file\n",
    "\n",
    "# Read the video frames\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # FACE\n",
    "        # mp_drawing.draw_landmarks(frame, results.face_landmarks, mp.solutions.face_mesh.FACEMESH_CONTOURS)\n",
    "        # RIGHT HAND\n",
    "        mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        # LEFT HAND\n",
    "        mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        # POSE\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Convert the frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply adaptive thresholding to segment white regions\n",
    "        _, threshold = cv2.threshold(gray_frame, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours in the thresholded frame\n",
    "        contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Iterate over the contours and draw bounding rectangles around white lines\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame with bounding boxes\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removes everything except White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('./Dataset/Alldone/18_Kan_C.mp4')  # Replace 'video.mp4' with the path to your video file\n",
    "\n",
    "# Read the video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to HSV color space\n",
    "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define the lower and upper white color thresholds (in HSV)\n",
    "    lower_white = np.array([0, 0, 200])\n",
    "    upper_white = np.array([180, 30, 255])\n",
    "\n",
    "    # Create a mask for the white color range\n",
    "    mask = cv2.inRange(hsv_frame, lower_white, upper_white)\n",
    "\n",
    "    # Apply the mask to the frame to detect white regions\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    # Display the frame with white regions detected\n",
    "    cv2.imshow('Video', result)\n",
    "    \n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Image output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
